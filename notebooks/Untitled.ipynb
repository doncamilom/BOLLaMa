{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4f6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the logging level for urllib3 and openai modules\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e7199e-14b5-4f7a-8958-445eace3209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use the Graphein submodule graphein.protein.features.sequence.embeddings, you need to install: biovec \n",
      "biovec cannot be installed via conda\n",
      "DEBUG:matplotlib:matplotlib data path: /home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/matplotlib/mpl-data\n",
      "DEBUG:matplotlib:CONFIGDIR=/home/rankovic/.config/matplotlib\n",
      "DEBUG:matplotlib:interactive is False\n",
      "DEBUG:matplotlib:platform is linux\n",
      "DEBUG:matplotlib:CACHEDIR=/home/rankovic/.cache/matplotlib\n",
      "DEBUG:matplotlib.font_manager:Using fontManager instance from /home/rankovic/.cache/matplotlib/fontlist-v330.json\n",
      "WARNING:graphein.protein.visualisation:To use the Graphein submodule graphein.protein.visualisation, you need to install: pytorch3d \n",
      "To do so, use the following command: conda install -c pytorch3d pytorch3d\n",
      "WARNING:graphein.protein.meshes:To use the Graphein submodule graphein.protein.meshes, you need to install: pytorch3d \n",
      "To do so, use the following command: conda install -c pytorch3d pytorch3d\n",
      "INFO:rdkit:Enabling RDKit 2022.09.5 jupyter extensions\n",
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=/home/rankovic/projects/BOLLaMa/notebooks, universal_newlines=False, shell=None, istream=None)\n",
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=/home/rankovic/projects/BOLLaMa/notebooks, universal_newlines=False, shell=None, istream=None)\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpwsrkuxsy\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpwsrkuxsy/_remote_module_non_scriptable.py\n",
      "INFO:lightning_lite.utilities.seed:Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bollama import BOAgent, BOTools\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"b2c71087533bf29e801f110250cee763724d68e0f1a5ad49bbdaddac04f11aff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462363ce-2096-483f-a514-7804e0241855",
   "metadata": {},
   "outputs": [],
   "source": [
    "carlos_the_frustrated_chemist = BOAgent(\n",
    "    BOTools().botools,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b263c6-f267-4a77-b1ae-91ec2d25a43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Get initial experiments.\n",
      "Action Input: 4\u001b[0m[427] baseline reaction index\n",
      "[137, 437, 20, 613]  indices before \n",
      "[137, 438, 20, 614] indices after\n",
      "Selected reactions: [137, 438, 20, 614]\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m['CCCC(CO)O', 'c1ccc2c(c1)nnn2C(=N)n3c4ccccc4nn3', 'C=C[C@H]1CN2CC[C@H]1C[C@@H]2[C@H](c3ccnc4c3cccc4)O', 'c1[nH]c2c(n1)c(=O)[nH]c(=O)[nH]2']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
      "Action: Get user input.\n",
      "Action Input: What was the result of the first experiment?\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m54679.3025, 13518.81469, 39731.89917, 26111.90483\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/botorch/models/utils/assorted.py:173: InputDataWarning: Input data is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  warnings.warn(msg, InputDataWarning)\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
      "Action: Propose next experiment.\n",
      "Action Input: {'CCCC(CO)O': 54679.3025, 'c1ccc2c(c1)nnn2C(=N)n3c4ccccc4nn3': 13518.81469, 'C=C[C@H]1CN2CC[C@H]1C[C@@H]2[C@H](c3ccnc4c3cccc4)O': 39731.89917, 'c1[nH]c2c(n1)c(=O)[nH]c(=O)[nH]2': 26111.90483}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbojana-rankovic\u001b[0m (\u001b[33mliac\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "DEBUG:git.cmd:Popen(['git', 'cat-file', '--batch-check'], cwd=/home/rankovic/projects/BOLLaMa, universal_newlines=False, shell=None, istream=<valid stream>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230326_011954-98wbkeoc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/liac/additives-report/runs/98wbkeoc\" target=\"_blank\">cerulean-bush-60</a></strong> to <a href=\"https://wandb.ai/liac/additives-report\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/liac/additives-report\" target=\"_blank\">https://wandb.ai/liac/additives-report</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/liac/additives-report/runs/98wbkeoc\" target=\"_blank\">https://wandb.ai/liac/additives-report/runs/98wbkeoc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:183: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GP   | 3     \n",
      "-------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d892843dd642008b45b90057f748f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([625]) best idxes\n",
      "ADDITIVE C(C(CO)CO)O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/botorch/models/utils/assorted.py:173: InputDataWarning: Input data is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  warnings.warn(msg, InputDataWarning)\n",
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('suggestion_idx', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6491c173abf34ae8a153a589ed029051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57633a07ec74bb78e5c313e01f63153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>▁</td></tr><tr><td>covar_module.raw_outputscale</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>likelihood.noise_covar.raw_noise</td><td>▁</td></tr><tr><td>likelihood_noise</td><td>▁</td></tr><tr><td>mean_module.raw_constant</td><td>▁</td></tr><tr><td>model_output_scale</td><td>▁</td></tr><tr><td>suggestion_idx</td><td>▁</td></tr><tr><td>sum_acq_values</td><td>▁</td></tr><tr><td>train/best_so_far</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>0.1</td></tr><tr><td>covar_module.raw_outputscale</td><td>-14.51017</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>global_optimum</td><td>86045.92668</td></tr><tr><td>likelihood.noise_covar.raw_noise</td><td>0.77382</td></tr><tr><td>likelihood_noise</td><td>0.77382</td></tr><tr><td>mean_module.raw_constant</td><td>0.0</td></tr><tr><td>model_output_scale</td><td>0.0</td></tr><tr><td>suggestion_idx</td><td>625.0</td></tr><tr><td>sum_acq_values</td><td>23929325.82881</td></tr><tr><td>top_10_count</td><td>0</td></tr><tr><td>top_1_count</td><td>0</td></tr><tr><td>top_3_count</td><td>0</td></tr><tr><td>top_5_count</td><td>0</td></tr><tr><td>train/best_so_far</td><td>54679.3025</td></tr><tr><td>trainer/global_step</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerulean-bush-60</strong> at: <a href=\"https://wandb.ai/liac/additives-report/runs/98wbkeoc\" target=\"_blank\">https://wandb.ai/liac/additives-report/runs/98wbkeoc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230326_011954-98wbkeoc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mC(C(CO)CO)O\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
      "Action: Get user input.\n",
      "Action Input: What was the result of the second experiment?\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m60609.38117\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/botorch/models/utils/assorted.py:173: InputDataWarning: Input data is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  warnings.warn(msg, InputDataWarning)\n",
      "DEBUG:git.cmd:Popen(['git', 'cat-file', '--batch-check'], cwd=/home/rankovic/projects/BOLLaMa, universal_newlines=False, shell=None, istream=<valid stream>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
      "Action: Propose next experiment.\n",
      "Action Input: {'CCCC(CO)O': 54679.3025, 'c1ccc2c(c1)nnn2C(=N)n3c4ccccc4nn3': 13518.81469, 'C=C[C@H]1CN2CC[C@H]1C[C@@H]2[C@H](c3ccnc4c3cccc4)O': 39731.89917, 'c1[nH]c2c(n1)c(=O)[nH]c(=O)[nH]2': 26111.90483, 'C(C(CO)CO)O': 60609.38117}\u001b[0m"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230326_012039-iwu24qo7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/liac/additives-report/runs/iwu24qo7\" target=\"_blank\">dulcet-eon-61</a></strong> to <a href=\"https://wandb.ai/liac/additives-report\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/liac/additives-report\" target=\"_blank\">https://wandb.ai/liac/additives-report</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/liac/additives-report/runs/iwu24qo7\" target=\"_blank\">https://wandb.ai/liac/additives-report/runs/iwu24qo7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:183: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GP   | 3     \n",
      "-------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007364efd80d47699f39104950ceb923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([91]) best idxes\n",
      "ADDITIVE CC(C)(CO)CO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/botorch/models/utils/assorted.py:173: InputDataWarning: Input data is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce66be7af114a658656b7e88a98fd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>▁</td></tr><tr><td>covar_module.raw_outputscale</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>likelihood.noise_covar.raw_noise</td><td>▁</td></tr><tr><td>likelihood_noise</td><td>▁</td></tr><tr><td>mean_module.raw_constant</td><td>▁</td></tr><tr><td>model_output_scale</td><td>▁</td></tr><tr><td>suggestion_idx</td><td>▁</td></tr><tr><td>sum_acq_values</td><td>▁</td></tr><tr><td>train/best_so_far</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>0.1</td></tr><tr><td>covar_module.raw_outputscale</td><td>-0.2131</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>global_optimum</td><td>86045.92668</td></tr><tr><td>likelihood.noise_covar.raw_noise</td><td>0.08735</td></tr><tr><td>likelihood_noise</td><td>0.08735</td></tr><tr><td>mean_module.raw_constant</td><td>-0.03434</td></tr><tr><td>model_output_scale</td><td>0.59226</td></tr><tr><td>suggestion_idx</td><td>91.0</td></tr><tr><td>sum_acq_values</td><td>29511328.18514</td></tr><tr><td>top_10_count</td><td>0</td></tr><tr><td>top_1_count</td><td>0</td></tr><tr><td>top_3_count</td><td>0</td></tr><tr><td>top_5_count</td><td>0</td></tr><tr><td>train/best_so_far</td><td>60609.38117</td></tr><tr><td>trainer/global_step</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dulcet-eon-61</strong> at: <a href=\"https://wandb.ai/liac/additives-report/runs/iwu24qo7\" target=\"_blank\">https://wandb.ai/liac/additives-report/runs/iwu24qo7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230326_012039-iwu24qo7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mCC(C)(CO)CO\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
      "Action: Get user input.\n",
      "Action Input: What was the result of the third experiment?\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m64968.11402\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/botorch/models/utils/assorted.py:173: InputDataWarning: Input data is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  warnings.warn(msg, InputDataWarning)\n",
      "DEBUG:git.cmd:Popen(['git', 'cat-file', '--batch-check'], cwd=/home/rankovic/projects/BOLLaMa, universal_newlines=False, shell=None, istream=<valid stream>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
      "Action: Propose next experiment.\n",
      "Action Input: {'CCCC(CO)O': 54679.3025, 'c1ccc2c(c1)nnn2C(=N)n3c4ccccc4nn3': 13518.81469, 'C=C[C@H]1CN2CC[C@H]1C[C@@H]2[C@H](c3ccnc4c3cccc4)O': 39731.89917, 'c1[nH]c2c(n1)c(=O)[nH]c(=O)[nH]2': 26111.90483, 'C(C(CO)CO)O': 60609.38117, 'CC(C)(CO)CO': 64968.11402}\u001b[0m"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230326_012132-w64azl75</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/liac/additives-report/runs/w64azl75\" target=\"_blank\">rural-tree-62</a></strong> to <a href=\"https://wandb.ai/liac/additives-report\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/liac/additives-report\" target=\"_blank\">https://wandb.ai/liac/additives-report</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/liac/additives-report/runs/w64azl75\" target=\"_blank\">https://wandb.ai/liac/additives-report/runs/w64azl75</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:183: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GP   | 3     \n",
      "-------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8f606591054aa2b55200ddfbd9ad2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([94]) best idxes\n",
      "ADDITIVE CC(C)(CCO)O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/botorch/models/utils/assorted.py:173: InputDataWarning: Input data is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788032ad7f1e4971b97fecc448fa073a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f672af33a5c44b4dad87467e29c28113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>▁</td></tr><tr><td>covar_module.raw_outputscale</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>likelihood.noise_covar.raw_noise</td><td>▁</td></tr><tr><td>likelihood_noise</td><td>▁</td></tr><tr><td>mean_module.raw_constant</td><td>▁</td></tr><tr><td>model_output_scale</td><td>▁</td></tr><tr><td>suggestion_idx</td><td>▁</td></tr><tr><td>sum_acq_values</td><td>▁</td></tr><tr><td>train/best_so_far</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>0.1</td></tr><tr><td>covar_module.raw_outputscale</td><td>-0.23964</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>global_optimum</td><td>86045.92668</td></tr><tr><td>likelihood.noise_covar.raw_noise</td><td>0.04519</td></tr><tr><td>likelihood_noise</td><td>0.04519</td></tr><tr><td>mean_module.raw_constant</td><td>-0.13601</td></tr><tr><td>model_output_scale</td><td>0.58049</td></tr><tr><td>suggestion_idx</td><td>94.0</td></tr><tr><td>sum_acq_values</td><td>31054931.03972</td></tr><tr><td>top_10_count</td><td>1</td></tr><tr><td>top_1_count</td><td>0</td></tr><tr><td>top_3_count</td><td>0</td></tr><tr><td>top_5_count</td><td>0</td></tr><tr><td>train/best_so_far</td><td>64968.11402</td></tr><tr><td>trainer/global_step</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rural-tree-62</strong> at: <a href=\"https://wandb.ai/liac/additives-report/runs/w64azl75\" target=\"_blank\">https://wandb.ai/liac/additives-report/runs/w64azl75</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230326_012132-w64azl75/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mCC(C)(CCO)O\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
      "Action: Get user input.\n",
      "Action Input: What was the result of the fourth experiment?\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m57565.50571\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/botorch/models/utils/assorted.py:173: InputDataWarning: Input data is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  warnings.warn(msg, InputDataWarning)\n",
      "DEBUG:git.cmd:Popen(['git', 'cat-file', '--batch-check'], cwd=/home/rankovic/projects/BOLLaMa, universal_newlines=False, shell=None, istream=<valid stream>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
      "Action: Propose next experiment.\n",
      "Action Input: {'CCCC(CO)O': 54679.3025, 'c1ccc2c(c1)nnn2C(=N)n3c4ccccc4nn3': 13518.81469, 'C=C[C@H]1CN2CC[C@H]1C[C@@H]2[C@H](c3ccnc4c3cccc4)O': 39731.89917, 'c1[nH]c2c(n1)c(=O)[nH]c(=O)[nH]2': 26111.90483, 'C(C(CO)CO)O': 60609.38117, 'CC(C)(CO)CO': 64968.11402, 'CC(C)(CCO)O': 57565.50571}\u001b[0m"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230326_012220-1bdkces2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/liac/additives-report/runs/1bdkces2\" target=\"_blank\">spring-sea-63</a></strong> to <a href=\"https://wandb.ai/liac/additives-report\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/liac/additives-report\" target=\"_blank\">https://wandb.ai/liac/additives-report</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/liac/additives-report/runs/1bdkces2\" target=\"_blank\">https://wandb.ai/liac/additives-report/runs/1bdkces2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:183: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GP   | 3     \n",
      "-------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daadf05a593f446a9f3d51ff2ccbd043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([100]) best idxes\n",
      "ADDITIVE CC(CCCO)O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/botorch/models/utils/assorted.py:173: InputDataWarning: Input data is not contained to the unit cube. Please consider min-max scaling the input data.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe11fc3a4bc448ab760590ca009b373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac0c82f0f564d6eb86e214f0dca8acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>▁</td></tr><tr><td>covar_module.raw_outputscale</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>likelihood.noise_covar.raw_noise</td><td>▁</td></tr><tr><td>likelihood_noise</td><td>▁</td></tr><tr><td>mean_module.raw_constant</td><td>▁</td></tr><tr><td>model_output_scale</td><td>▁</td></tr><tr><td>suggestion_idx</td><td>▁</td></tr><tr><td>sum_acq_values</td><td>▁</td></tr><tr><td>train/best_so_far</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>0.1</td></tr><tr><td>covar_module.raw_outputscale</td><td>-0.27277</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>global_optimum</td><td>86045.92668</td></tr><tr><td>likelihood.noise_covar.raw_noise</td><td>0.03085</td></tr><tr><td>likelihood_noise</td><td>0.03085</td></tr><tr><td>mean_module.raw_constant</td><td>-0.22064</td></tr><tr><td>model_output_scale</td><td>0.56604</td></tr><tr><td>suggestion_idx</td><td>100.0</td></tr><tr><td>sum_acq_values</td><td>31115489.02452</td></tr><tr><td>top_10_count</td><td>1</td></tr><tr><td>top_1_count</td><td>0</td></tr><tr><td>top_3_count</td><td>0</td></tr><tr><td>top_5_count</td><td>0</td></tr><tr><td>train/best_so_far</td><td>64968.11402</td></tr><tr><td>trainer/global_step</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-sea-63</strong> at: <a href=\"https://wandb.ai/liac/additives-report/runs/1bdkces2\" target=\"_blank\">https://wandb.ai/liac/additives-report/runs/1bdkces2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230326_012220-1bdkces2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mCC(CCCO)O\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: Based on the results of the initial experiments, it seems like the fourth experiment had the lowest output value, so it may be worth exploring that direction further. However, it's important to keep in mind that there may be other factors to consider when optimizing the additives for this chemical reaction. As for the next step, the proposed experiment was CC(CCCO)O. What do you think?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the results of the initial experiments, it seems like the fourth experiment had the lowest output value, so it may be worth exploring that direction further. However, it's important to keep in mind that there may be other factors to consider when optimizing the additives for this chemical reaction. As for the next step, the proposed experiment was CC(CCCO)O. What do you think?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Hello gpt. I want to optimize the additives for a chemical reaction. \"\n",
    "    \"Can you please suggest what experiments to do first, \"\n",
    "    \"and then let's do multiple optimization steps until I am satisfied.\"\n",
    "    \"Let's start with 4 experiments.\"\n",
    "    \n",
    "    \"Everytime you propose an experiment, ask for my input to know the result of the experiment.\"\n",
    ")\n",
    "\n",
    "carlos_the_frustrated_chemist.run({\"input\":{\"user\": prompt}, \"chat_history\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cef74-5bce-486e-ade8-88d8224d151e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6bb798-8996-408b-aa7a-810bfa1e778b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A single string input was passed in, but this chain expects multiple inputs ({'input', 'chat_history'}). When a chain expects multiple inputs, please call it by passing in a dictionary, eg `chain({'foo': 1, 'bar': 2})`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m carlos_the_frustrated_chemist \u001b[38;5;241m=\u001b[39m BOAgent(\n\u001b[1;32m      2\u001b[0m     BOTools()\u001b[38;5;241m.\u001b[39mbotools,\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello gpt. I want to optimize the additives for a chemical reaction. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you please suggest what experiments to do first, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEverytime you propose an experiment, ask for my input to know the result of the experiment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mcarlos_the_frustrated_chemist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/bollama/src/bollama/agents.py:44\u001b[0m, in \u001b[0;36mBOAgent.run\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/additive_bo/lib/python3.8/site-packages/langchain/chains/base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/envs/additive_bo/lib/python3.8/site-packages/langchain/chains/base.py:106\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m, inputs: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any], return_only_outputs: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     94\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124;03m\"\"\"Run the logic of this chain and add to output if desired.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m         inputs,\n\u001b[1;32m    110\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m     )\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/additive_bo/lib/python3.8/site-packages/langchain/chains/base.py:185\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         _input_keys \u001b[38;5;241m=\u001b[39m _input_keys\u001b[38;5;241m.\u001b[39mdifference(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mmemory_variables)\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_input_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA single string input was passed in, but this chain expects \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple inputs (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_input_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). When a chain expects \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple inputs, please call it by passing in a dictionary, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meg `chain(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfoo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: 1, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: 2})`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         )\n\u001b[1;32m    191\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mlist\u001b[39m(_input_keys)[\u001b[38;5;241m0\u001b[39m]: inputs}\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: A single string input was passed in, but this chain expects multiple inputs ({'input', 'chat_history'}). When a chain expects multiple inputs, please call it by passing in a dictionary, eg `chain({'foo': 1, 'bar': 2})`"
     ]
    }
   ],
   "source": [
    "carlos_the_frustrated_chemist = BOAgent(\n",
    "    BOTools().botools,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    \"Hello gpt. I want to optimize the additives for a chemical reaction. \"\n",
    "    \"Can you please suggest what experiments to do first, \"\n",
    "    \"and then let's do multiple optimization steps until I am satisfied.\"\n",
    "    \"Let's start with 4 experiments.\"\n",
    "    \n",
    "    \"Everytime you propose an experiment, ask for my input to know the result of the experiment.\"\n",
    ")\n",
    "\n",
    "carlos_the_frustrated_chemist.run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d0563cc-1615-42ea-95b9-2356f19edaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "additive_bo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c8fd31767018617c963c6386c67ff85ad8a2ec47e81bbb467830b2728b4f39b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
